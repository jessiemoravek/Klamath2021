---
title: "2021_KlamathMain_Visualization"
output: html_document
---


```{r, include=F}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(patchwork)
library(astsa)
library(forecast)
library(dplyr)
library(pander)
library(MARSS)
library(MASS)
library(gridExtra)
library(weathermetrics)
library(ggplot2)
library(zoo)
library(imputeTS) 
library(WaveletComp)
library(magick)
```


#Read in Data: Seiad Valley
```{r}
##Reading in a 6 year 15 or 30 min temperature time series dataset from Klamath River at Seiad Valley called KSV(Klamath Seiad Valley). Data collected by Karuk Tribe
KSV <- read.csv("SeiadValley_KlamathMain_AllData.csv")
KSV$date <- lubridate::mdy_hm(KSV$Date_Time)#convert dates to POSIXct format 

#Trim the dataset
KSV <- KSV[c(2585:117486),] #Most missing data is between 2015-Feb 2016, so removing the first ~ year of data to make dataset less sketchy

#Check for missing data
missing_data <- KSV[!complete.cases(KSV),] 
missing_data 
#Need a complete dataset with no missing data. Since there is missing data in this dataset, we will need to interpolate

#Bin data by hour and average temperature recordings to the hourly level
KSV$hour <- lubridate::floor_date(KSV$date, unit="hour") #Before we interpolate, let's bin by hour
KSV_hourly <- KSV %>% #Summarize recordings to the hourly level (we have a mix of 30 min and 15 min readings)
  group_by(hour) %>% 
  summarize(mean_temp=mean(Temp))

head(KSV_hourly) #check the dataset start date, use for "hour" sequence
tail(KSV_hourly) #check the dataset end date, use for "hour" sequence

#Create hourly sequence to ensure all missing data is accounted for
hour <- seq(mdy_h('5/2/2016 15'),mdy_h('2/1/2022 13'),by = "hour") #Create an object that goes hour by hour for the entire time series to ensure that ALL missing data is accounted for
hour <- as.data.frame(hour) #convert "hour" to data frame
KSV_hourly <- left_join(hour, KSV_hourly) #left join hour and dataset

#Convert NaNs to NAs
KSV_hourly$mean_temp[KSV_hourly$mean_temp == "NaN"] <- NA

#Double check missing data
missing_data <- KSV_hourly[!complete.cases(KSV_hourly),] 
missing_data #Now we are sure that all the missing hour time steps are included.

#z score to control for outliers
KSV_hourly$zTemp <- zscore(KSV_hourly$mean_temp)

#Convert to time series
KSV_ts <- ts(KSV_hourly$zTemp, start = c(123, 15), frequency = 24) # This time series starts on 2 May 2016 at 2 am, so it starts on day 123 (leap year) at hour 15 and the frequency is 24 (24 hours per day)
#^^^This is very confusing and I still don't fully understand how to convert data to time series so may want to ask Albert for clarification. 
ts.plot(KSV_ts,main="Temperature",ylab = "Temperature (C)", xlab = "Time")
```
#Read in Data: Orleans
```{r}
##Reading in a 6 year 15 or 30 min temperature time series dataset from Klamath River at Seiad Valley called KSV(Klamath Seiad Valley). Data collected by Karuk Tribe
KO <- read.csv("Orleans_KlamathMain_AllData.csv")
KO$date <- lubridate::mdy_hm(KO$Date_Time)#convert dates to POSIXct format 

#Trim the dataset
KO <- KO[c(4190:135357),] #Removing the first ~ year of data to align with Seiad Valley dataset

#Check for missing data
missing_data <- KO[!complete.cases(KO),] 
missing_data 
#Need a complete dataset with no missing data. Since there is missing data in this dataset, we will need to interpolate

#Bin data by hour and average temperature recordings to the hourly level
KO$hour <- lubridate::floor_date(KO$date, unit="hour") #Before we interpolate, let's bin by hour
KO_hourly <- KO %>% #Summarize recordings to the hourly level (we have a mix of 30 min and 15 min readings)
  group_by(hour) %>% 
  summarize(mean_temp=mean(Temp))

head(KO_hourly) #check the dataset start date, use for "hour" sequence
tail(KO_hourly) #check the dataset end date, use for "hour" sequence

#Create hourly sequence to ensure all missing data is accounted for
hour <- seq(mdy_h('5/2/2016 15'),mdy_h('2/1/2022 13'),by = "hour") #Create an object that goes hour by hour for the entire time series to ensure that ALL missing data is accounted for
hour <- as.data.frame(hour) #convert "hour" to data frame
KO_hourly <- left_join(hour, KO_hourly) #left join hour and dataset

#Convert NaNs to NAs
KO_hourly$mean_temp[KO_hourly$mean_temp == "NaN"] <- NA

#Double check missing data
missing_data <- KO_hourly[!complete.cases(KO_hourly),] 
missing_data #Now we are sure that all the missing hour time steps are included.

#z score to control for outliers
KO_hourly$zTemp <- zscore(KO_hourly$mean_temp)

#Convert to time series
KO_ts <- ts(KO_hourly$zTemp, start = c(123, 15), frequency = 24) # This time series starts on 2 May 2016 at 2 am, so it starts on day 123 (leap year) at hour 15 and the frequency is 24 (24 hours per day)
#^^^This is very confusing and I still don't fully understand how to convert data to time series so may want to ask Albert for clarification. 
ts.plot(KO_ts,main="Temperature",ylab = "Temperature (C)", xlab = "Time")
```
#Compare Orleans and Seiad Valley datasets
```{r}
png("Fig_Klamath_OrleansVsSeiadCk.png", width = 1000, height = 1000)
plot(KSV_hourly$mean_temp,KO_hourly$mean_temp,abline(a=0, b=1, col="red"),pch = ".")
dev.off()

```
#MARSS Model
##Format data
```{r}
#KSV (Seiad Valley)
#Bin by day
KSV_hourly$day <-lubridate::floor_date(KSV_hourly$hour, unit="day") 
head(KSV_hourly)
tail(KSV_hourly)

#Group by day and calculate mean
KSV_daily <- KSV_hourly %>% 
  group_by(day) %>% 
  summarize(mean_temp=mean(mean_temp))

#KO (Orleans)
#Bin by day
KO_hourly$day <-lubridate::floor_date(KO_hourly$hour, unit="day") 
head(KO_hourly)
tail(KO_hourly)

#Group by day and calculate mean
KO_daily <- KO_hourly %>% 
  group_by(day) %>% 
  summarize(mean_temp=mean(mean_temp))

#Make a data frame
daily_means <- cbind(day = KSV$day, KSV = KSV_daily$mean_temp, KO = KO_daily$mean_temp)
daily_means <- as.data.frame(daily_means)

daily_means_long <- t(daily_means[,-c(1,2)]) #Take the daily means and put them into a dataframe formatted for MARSS (not a matrix yet)
dim(daily_means_long) #check we did that right
covariate <- t(daily_means[,c(2)]) #Do the same for the covariates

```
##Data Matrix 
```{r, include = F}
#Convert data to matrix and z-score
transformed_dat <- as.matrix(daily_means_long)
transformed_dat <- zscore(transformed_dat)

```

##Covariates
```{r, include=FALSE}
#Build the little c matrix, call it matrixc
dim(covariate)
matrixc <- matrix(nrow=1,ncol=2102)
matrixc <- (as.matrix(covariate))
```

###Check data and covariates
```{r, include = F}
matplot(t(transformed_dat))
matplot(t(matrixc))

```
##Z Matrices
```{r}

```


#Interpolate missing data
```{r, cache = TRUE}
#Run ARIMA to interpolate missing data
y <- KSV_ts
date_s <- KSV_hourly$hour
y_na <- ifelse(is.na(y),0,NA)

fit <- auto.arima(y,trace=TRUE) #fit limited number of models (faster)
saveRDS(fit,"KlamathSV_ARIMA_fit.rds")
summary(fit) #Take a closer look at the best fitted model
forecast_fit <- forecast(y,model=fit) #Predict values using the calibration dataset
saveRDS(forecast_fit,"KlamathSV_ARIMA_forecast_fit.rds")
```

#Plot interpolated data
```{r}
#Plot the observed and interpolated temperature regimes
par(mar=c(2,4,1,1)) 
plot(date_s,y,xlab="Time",ylab="Temp",lwd=2,type="l") 
lines(date_s,forecast_fit$fitted,col="steelblue")

#Plot predicted versus observed values
scatter.smooth(y,forecast_fit$fitted,xlab="Observed",ylab="Predicted",pch=".",main = "Temperature")
abline(0,1,lty=2)
R2 = round(cor.test(y,forecast_fit$fitted,na.rm=T)$estimate^2,2)
mtext(side=3,line=-2,adj=0.1,bquote(R^2 == .(R2)))
```

#Check residuals of interpolated data
```{r}
#Check residuals of the interpolation process
checkresiduals(fit) #also gives the results for the Ljung_Box test with H0 = randomly distributed errors (white noise)

#plot residuals versus fitted values (=check for heterosedasticity); if problems try to transform the data 
par(mar=c(4,4,4,4))
scatter.smooth(forecast_fit$fitted,forecast_fit$residuals,pch = ".",ylab="Residuals",xlab="Fitted values")
```

#Smooth interpolated data
##NOTE: This chunk takes FOREVER beware
```{r, cache = TRUE}
#Interpolate missing values using a Kalman filter (=smoother)
y_inter <- na_kalman(y,model=fit$model) #use the fitted model
saveRDS(y_inter,"KlamathSV_ARIMA_y_inter.rds")

#Plot the results
par(mar=c(2,4,1,1))
plot(y_inter,xlab="",ylab="Temperature (C)",col="steelblue",main="Interpolation missing values")
lines(y,col="black")

```

#Format interpolated time series into a dataframe
```{r}
#Put the interpolated temperature dataset (y_inter) into a dataset with the correct dates (this does not work using the time series object because I had to designate hourly steps when making the time series, so the dates are messed up. There is probably a better way to do this...). 
x <- as.data.frame(y_inter) #change y_inter from a ts to a dataframe
x$ID <- seq.int(nrow(x)) #add a unique ID, check it is correct length
date <- as.data.frame(APH$hour) #make the unique "hour" index from APH into a separate dataframe
date$ID <- seq.int(nrow(date)) #give that a unique ID that aligns with x
y_inter_df <- merge(x,date,"ID") #merge the two dataframes
colnames(y_inter_df)<-c("ID","temp","date") #rename columns
```