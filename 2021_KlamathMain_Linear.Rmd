---
title: "2021_KlamathMain_Linear"
output: html_document
---

```{r, include=F}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(patchwork)
library(astsa)
library(forecast)
library(dplyr)
library(pander)
library(MARSS)
library(MASS)
library(gridExtra)
library(weathermetrics)
library(ggplot2)
library(zoo)
library(imputeTS) 
library(WaveletComp)
library(magick)
```


#Read in Data: Seiad Valley
```{r}
##Reading in a 6 year 15 or 30 min temperature time series dataset from Klamath River at Seiad Valley called KSV(Klamath Seiad Valley). Data collected by Karuk Tribe
KSV <- read.csv("SeiadValley_KlamathMain_AllData.csv")
KSV$date <- lubridate::mdy_hm(KSV$Date_Time)#convert dates to POSIXct format 

#Trim the dataset
KSV <- KSV[c(2585:117486),] #Most missing data is between 2015-Feb 2016, so removing the first ~ year of data to make dataset less sketchy

#Check for missing data
missing_data <- KSV[!complete.cases(KSV),] 
missing_data 
#Need a complete dataset with no missing data. Since there is missing data in this dataset, we will need to interpolate

#Bin data by hour and average temperature recordings to the hourly level
KSV$hour <- lubridate::floor_date(KSV$date, unit="hour") #Before we interpolate, let's bin by hour
KSV_hourly <- KSV %>% #Summarize recordings to the hourly level (we have a mix of 30 min and 15 min readings)
  group_by(hour) %>% 
  summarize(mean_temp=mean(Temp))

head(KSV_hourly) #check the dataset start date, use for "hour" sequence
tail(KSV_hourly) #check the dataset end date, use for "hour" sequence

#Create hourly sequence to ensure all missing data is accounted for
hour <- seq(mdy_h('5/2/2016 15'),mdy_h('2/1/2022 13'),by = "hour") #Create an object that goes hour by hour for the entire time series to ensure that ALL missing data is accounted for
hour <- as.data.frame(hour) #convert "hour" to data frame
KSV_hourly <- left_join(hour, KSV_hourly) #left join hour and dataset

#Convert NaNs to NAs
KSV_hourly$mean_temp[KSV_hourly$mean_temp == "NaN"] <- NA

#Double check missing data
missing_data <- KSV_hourly[!complete.cases(KSV_hourly),] 
missing_data #Now we are sure that all the missing hour time steps are included.

#z score to control for outliers
KSV_hourly$zTemp <- zscore(KSV_hourly$mean_temp)

#Convert to time series
KSV_ts <- ts(KSV_hourly$zTemp, start = c(123, 15), frequency = 24) # This time series starts on 2 May 2016 at 2 am, so it starts on day 123 (leap year) at hour 15 and the frequency is 24 (24 hours per day)
#^^^This is very confusing and I still don't fully understand how to convert data to time series so may want to ask Albert for clarification. 
ts.plot(KSV_ts,main="Temperature",ylab = "Temperature (C)", xlab = "Time")
```
#Read in Data: Orleans
```{r}
##Reading in a 6 year 15 or 30 min temperature time series dataset from Klamath River at Seiad Valley called KSV(Klamath Seiad Valley). Data collected by Karuk Tribe
KO <- read.csv("Orleans_KlamathMain_AllData.csv")
KO$date <- lubridate::mdy_hm(KO$Date_Time)#convert dates to POSIXct format 

#Trim the dataset
KO <- KO[c(4190:135357),] #Removing the first ~ year of data to align with Seiad Valley dataset

#Check for missing data
missing_data <- KO[!complete.cases(KO),] 
missing_data 
#Need a complete dataset with no missing data. Since there is missing data in this dataset, we will need to interpolate

#Bin data by hour and average temperature recordings to the hourly level
KO$hour <- lubridate::floor_date(KO$date, unit="hour") #Before we interpolate, let's bin by hour
KO_hourly <- KO %>% #Summarize recordings to the hourly level (we have a mix of 30 min and 15 min readings)
  group_by(hour) %>% 
  summarize(mean_temp=mean(Temp))

head(KO_hourly) #check the dataset start date, use for "hour" sequence
tail(KO_hourly) #check the dataset end date, use for "hour" sequence

#Create hourly sequence to ensure all missing data is accounted for
hour <- seq(mdy_h('5/2/2016 15'),mdy_h('2/1/2022 13'),by = "hour") #Create an object that goes hour by hour for the entire time series to ensure that ALL missing data is accounted for
hour <- as.data.frame(hour) #convert "hour" to data frame
KO_hourly <- left_join(hour, KO_hourly) #left join hour and dataset

#Convert NaNs to NAs
KO_hourly$mean_temp[KO_hourly$mean_temp == "NaN"] <- NA

#Double check missing data
missing_data <- KO_hourly[!complete.cases(KO_hourly),] 
missing_data #Now we are sure that all the missing hour time steps are included.

#z score to control for outliers
KO_hourly$zTemp <- zscore(KO_hourly$mean_temp)

#Convert to time series
KO_ts <- ts(KO_hourly$zTemp, start = c(123, 15), frequency = 24) # This time series starts on 2 May 2016 at 2 am, so it starts on day 123 (leap year) at hour 15 and the frequency is 24 (24 hours per day)
#^^^This is very confusing and I still don't fully understand how to convert data to time series so may want to ask Albert for clarification. 
ts.plot(KO_ts,main="Temperature",ylab = "Temperature (C)", xlab = "Time")
```


#Compare Orleans and Seiad Valley datasets
```{r}
plot(KSV_hourly$mean_temp,KO_hourly$mean_temp,abline(a=0, b=1, col="red"),pch = ".")

#Tightly correlated-- 0.99
cor(KSV_hourly$mean_temp, KO_hourly$mean_temp, use = "na.or.complete", method = "pearson")


ggplot()+
  geom_line(data = KSV_hourly, aes(x = hour, y = mean_temp, color = "SeiadValley"))+
  geom_line(data = KO_hourly, aes(x = hour, y = mean_temp, color = "Orleans"))+
  labs(x = "Year",
       y = "Temperature (C)")+
  theme_classic()+
  theme(text=element_text(size=12), legend.position = "bottom")+
  scale_colour_manual("", values = c("SeiadValley"="steelblue", "Orleans"="salmon")) +
  scale_y_continuous("Temperature (C)", limits = c(0,30), breaks = 5*0:30) 
```


#Trim Overlapping part of datasets
```{r}
#Start overlap: 2018-12-12 10:00:00 (from KO dataset, 22892)
#End overlap: 2020-05-08 10:00:00 (from KSV dataset, 35515)

KO_hourly_cut <- KO_hourly[c(22892:35516),]
KSV_hourly_cut <- KSV_hourly[c(22892:35516),] 

ggplot()+
  geom_line(data = KSV_hourly_cut, aes(x = hour, y = mean_temp, color = "SeiadValley"))+
  geom_line(data = KO_hourly_cut, aes(x = hour, y = mean_temp, color = "Orleans"))+
  labs(x = "Year",
       y = "Temperature (C)")+
  theme_classic()+
  theme(text=element_text(size=12), legend.position = "bottom")+
  scale_colour_manual("", values = c("SeiadValley"="steelblue", "Orleans"="salmon")) +
  scale_y_continuous("Temperature (C)", limits = c(0,30), breaks = 5*0:30)

```
#Check correlation between overlapping data points
```{r}
cor(KSV_hourly_cut$mean_temp, KO_hourly_cut$mean_temp, use = "na.or.complete", method = "pearson")
#correlation coefficient is 0.9921006
```
#Create a linear model
```{r}
#convert dataset to time series to ensure equal steps
KSV_cut_ts <- ts(KSV_hourly_cut$mean_temp, start = c(346, 10), frequency = 24)
KO_cut_ts <- ts(KO_hourly_cut$mean_temp, start = c(346, 10), frequency = 24)

fit_lm <- lm(KSV_cut_ts~KO_cut_ts, na.action=na.exclude)  # fit with na.exclude
fit_lm#So the equation y = mx+b is... KSV = 1.0617(KO) - 0.7997

```
#Cut missing KSV values
```{r}
#Start at 35515, end at 36959
KSV_hourly_missing<- KSV_hourly[c(35516:36958),]
KSV_missing_ts <- ts(KSV_hourly_missing$mean_temp, start = c(141, 10), frequency = 24)
str(KSV_missing_ts)

KSV_predict <- predict(fit_lm, newdata = KSV_missing_ts)
KSV_predict <- as.data.frame(KSV_predict)
KSV_predict$fit
str(KSV_predict)
KSV_hourly_missing$mean_temp <- KSV_predict$fit
```
#Plot missing KSV values
```{r}
ggplot()+
  geom_line(data = KSV_hourly, aes(x = hour, y = mean_temp, color = "SeiadValley"))+
  geom_line(data = KSV_predict, aes(x = , y = fit, color = "Orleans"))+
  labs(x = "Year",
       y = "Temperature (C)")+
  theme_classic()+
  theme(text=element_text(size=12), legend.position = "bottom")+
  scale_colour_manual("", values = c("SeiadValley"="steelblue", "Orleans"="salmon")) +
  scale_y_continuous("Temperature (C)", limits = c(0,30), breaks = 5*0:30)

```


